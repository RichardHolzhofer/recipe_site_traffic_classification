# Recipe Site Traffic Classification Project

The marketing department needs a reliable way to identify popular recipes to increase website traffic. This project solves that problem by developing a binary classification model that predicts whether a recipe will drive high or low traffic, meeting their requirement of 80% precision.

---

## ğŸš€ Project Status and Results

The project successfully developed a classification model that predicts high-traffic recipes with a **precision of ~ 92%**. This solution provides a valuable tool for content strategy, far exceeding the marketing department's 80% precision requirement.

**Key Findings:**
* **Recipe Categories are Key Predictors:** The recipe's category is a strong indicator of traffic. Recipes in the **Vegetable** category are a strong driver of high traffic.
* **Categories to Avoid:** The marketing team should be cautious when posting recipes from the **Beverages, Breakfast**, or **Chicken** categories, as they are less likely to result in high traffic.
* **Nutritional Information:** The model found no strong correlation between a recipe's nutritional content and its traffic performance.

The final model is currently deployed on the following URL: [https://recipesitetrafficclassification.streamlit.app](https://recipesitetrafficclassification.streamlit.app) where both single and batch predictions are available.

---

## ğŸ› ï¸ Tech Stack and Project Structure

### Tech Stack
* **Machine Learning:** Python, Scikit-learn, XGBoost, CatBoost
* **MLOps:** DVC, Dagshub, MLflow
* **Data Validation:** Great Expectations, Evidently
* **Deployment:** Streamlit, FastAPI, Docker, AWS S3, AWS ECR, AWS EC2
* **CI/CD:** GitHub Actions, Ruff, Pytest
* **Databases:** MongoDB

### Folder Tree

â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cicd.yaml
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml.example
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â””â”€â”€ raw_s3/
â”œâ”€â”€ data_schema/
â”‚   â””â”€â”€ schema.yaml
â”œâ”€â”€ final_model/
â”‚   â””â”€â”€ final_model_with_preprocessor.joblib
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_EDA.ipynb
â”‚   â””â”€â”€ 2_Model_Training.ipynb
â”œâ”€â”€ recipesitetraffic/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ constants/
â”‚   â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ exception/
â”‚   â”œâ”€â”€ logging/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ table.html
â”‚   â””â”€â”€ upload_form.html
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_ingestion.py
â”‚   â”œâ”€â”€ test_data_validation.py
â”‚   â”œâ”€â”€ test_data_transformation.py
â”‚   â””â”€â”€ test_model_trainer.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ streamlit_app.py
â””â”€â”€ template.py

---

## âš™ï¸ Development and Pipelines

### ETL & Training Pipelines
* An ETL pipeline (`etl_pipeline.py`) extracts data from AWS S3, converts it to JSON format, and uploads it to MongoDB.
* The training pipeline consists of **Data Ingestion, Validation, Transformation, and Model Training**.
* **Data Validation** is performed using **Great Expectations** for schema checks and **Evidently** for data drift.
* **MLflow** is used for experiment tracking, with **Dagshub** serving as the MLflow server.
* The final model artifact, including the preprocessor, is saved and stored on **AWS S3**.

### Deployment & CI/CD
* The project includes a **Dockerized FastAPI application** for batch predictions, exposed on `0.0.0.0:8000`.
* A `cicd.yaml` file automates a CI/CD pipeline using **GitHub Actions**. This pipeline performs **code linting with Ruff**, runs **tests with Pytest**, and automates **deployment to AWS** via pushing the Docker image to **AWS ECR** and deploying it to an **AWS EC2** instance.

---

## ğŸ“§ Contact

If you have any questions, feel free to reach out to me at richard.holzhofer@gmail.com.

Thanks for reading!